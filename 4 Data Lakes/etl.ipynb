{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Read config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dl.cfg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Instantiate a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.5\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Extract log and song data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "with ZipFile(config['LOCAL']['INPUT_DATA'] + \"song-data.zip\", 'r') as song_zip:\n",
    "    song_zip.extractall(config['LOCAL']['INPUT_DATA'] + \"extract/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "with ZipFile(config['LOCAL']['INPUT_DATA'] + \"log-data.zip\", 'r') as log_zip:\n",
    "    log_zip.extractall(config['LOCAL']['INPUT_DATA'] + \"extract/log_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Load log and song data into data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_data = config['LOCAL']['INPUT_DATA'] + \"extract/song_data/*/*/*/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_data = config['LOCAL']['INPUT_DATA'] + \"extract/log_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_df = spark.read.json(song_data)\n",
    "song_df.createOrReplaceTempView(\"song\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|     artist_location|artist_longitude|         artist_name| duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|ARDR4AC1187FB371A1|           null|                    |            null|Montserrat Caball...|511.16363|        1|SOBAYLL12A8C138AF9|Sono andati? Fing...|   0|\n",
      "|AREBBGV1187FB523D2|           null|         Houston, TX|            null|Mike Jones (Featu...|173.66159|        1|SOOLYAZ12A6701F4A6|Laws Patrolling (...|   0|\n",
      "|ARMAC4T1187FB3FA4C|       40.82624|   Morris Plains, NJ|       -74.47995|The Dillinger Esc...|207.77751|        1|SOBBUGU12A8C13E95D|Setting Fire to S...|2004|\n",
      "|ARPBNLO1187FB3D52F|       40.71455|        New York, NY|       -74.00712|            Tiny Tim| 43.36281|        1|SOAOIBZ12AB01815BE|I Hold Your Hand ...|2000|\n",
      "|ARNF6401187FB57032|       40.79086|New York, NY [Man...|       -73.96644|   Sophie B. Hawkins|  305.162|        1|SONWXQJ12A8C134D94|The Ballad Of Sle...|1994|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_df.printSchema()\n",
    "song_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Transform timestamp column in log data frame and filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "get_datetime = udf(lambda ts: datetime.fromtimestamp(float(ts)/1000.).strftime('%Y-%m-%d %H:%M:%S'), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_df = spark.read.json(log_data)\n",
    "log_df = log_df.withColumn('start_time', get_datetime('ts')).filter(\"page='NextSong'\")\n",
    "log_df.createOrReplaceTempView(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- start_time: string (nullable = true)\n",
      "\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+-------------------+\n",
      "|     artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|                song|status|           ts|           userAgent|userId|         start_time|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+-------------------+\n",
      "|   Harmonia|Logged In|     Ryan|     M|            0|   Smith|655.77751| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|       Sehr kosmisch|   200|1542241826796|\"Mozilla/5.0 (X11...|    26|2018-11-15 00:30:26|\n",
      "|The Prodigy|Logged In|     Ryan|     M|            1|   Smith|260.07465| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|     The Big Gundown|   200|1542242481796|\"Mozilla/5.0 (X11...|    26|2018-11-15 00:41:21|\n",
      "|      Train|Logged In|     Ryan|     M|            2|   Smith|205.45261| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|            Marry Me|   200|1542242741796|\"Mozilla/5.0 (X11...|    26|2018-11-15 00:45:41|\n",
      "|Sony Wonder|Logged In|   Samuel|     M|            0|Gonzalez|218.06975| free|Houston-The Woodl...|   PUT|NextSong|1.540492941796E12|      597|           Blackbird|   200|1542253449796|\"Mozilla/5.0 (Mac...|    61|2018-11-15 03:44:09|\n",
      "|  Van Halen|Logged In|    Tegan|     F|            2|  Levine|289.38404| paid|Portland-South Po...|   PUT|NextSong|1.540794356796E12|      602|Best Of Both Worl...|   200|1542260935796|\"Mozilla/5.0 (Mac...|    80|2018-11-15 05:48:55|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.printSchema()\n",
    "log_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Write songplay table partitioned by year and artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------+-----+------------------+------------------+----------+--------------------+--------------------+\n",
      "|songplay_id|         start_time|user_id|level|           song_id|         artist_id|session_id|            location|          user_agent|\n",
      "+-----------+-------------------+-------+-----+------------------+------------------+----------+--------------------+--------------------+\n",
      "|          1|2018-11-21 21:56:47|     15| paid|SOZCTXZ12AB0182364|AR5KOSW1187FB35FF4|       818|Chicago-Napervill...|\"Mozilla/5.0 (X11...|\n",
      "+-----------+-------------------+-------+-----+------------------+------------------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplay_table = spark.sql(\"\"\"\n",
    "    select\n",
    "        row_number() over (partition by l.ts order by l.userId) as songplay_id,\n",
    "        l.start_time,\n",
    "        l.userId as user_id,\n",
    "        l.level,\n",
    "        s.song_id,\n",
    "        s.artist_id,\n",
    "        l.sessionId as session_id,\n",
    "        l.location,\n",
    "        l.userAgent as user_agent\n",
    "    from\n",
    "        song s\n",
    "    inner join log l on\n",
    "        s.artist_name=l.artist and\n",
    "        s.title=l.song\n",
    "\"\"\")\n",
    "\n",
    "songplay_table.show(10)\n",
    "\n",
    "try:\n",
    "    songplay_table.withColumn('year', year(col('start_time'))).write.partitionBy('year', 'artist_id').parquet(config['LOCAL']['OUTPUT_DATA'] + \"songplays.parquet\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Write user table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     98|    Jordyn|   Powell|     F| free|\n",
      "|     34|    Evelin|    Ayala|     F| free|\n",
      "|     85|   Kinsley|    Young|     F| paid|\n",
      "|     38|    Gianna|    Jones|     F| free|\n",
      "|     85|   Kinsley|    Young|     F| free|\n",
      "|     63|      Ayla|  Johnson|     F| free|\n",
      "|     37|    Jordan|    Hicks|     F| free|\n",
      "|      6|   Cecilia|    Owens|     F| free|\n",
      "|     15|      Lily|     Koch|     F| paid|\n",
      "|     27|    Carlos|   Carter|     M| free|\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_table = spark.sql(\"\"\"\n",
    "    select distinct\n",
    "        userId as user_id,\n",
    "        firstName as first_name,\n",
    "        lastName as last_name,\n",
    "        gender,\n",
    "        level\n",
    "    from\n",
    "        log\n",
    "\"\"\")\n",
    "\n",
    "user_table.show(10)\n",
    "\n",
    "try:\n",
    "    user_table.write.parquet(config['LOCAL']['OUTPUT_DATA'] + \"users.parquet\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Write song table partitioned by year and artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|               title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SOGNCJP12A58A80271|Do You Finally Ne...|ARB29H41187B98F0EF|1972|342.56934|\n",
      "|SOOJPRH12A8C141995|   Loaded Like A Gun|ARBGXIG122988F409D|   0|173.19138|\n",
      "|SOFCHDR12AB01866EF|         Living Hell|AREVWGE1187B9B890A|   0|282.43546|\n",
      "|SOWTBJW12AC468AC6E|Broken-Down Merry...|ARQGYP71187FB44566|   0|151.84934|\n",
      "|SOGOSOV12AF72A285E|   ¿Dónde va Chichi?|ARGUVEV1187B98BA17|1997|313.12934|\n",
      "|SOTUKVB12AB0181477|   Blessed Assurance|AR7ZKHQ1187B98DD73|1993|  270.602|\n",
      "|SOMVWWT12A58A7AE05|Knocked Out Of Th...|ARQ9BO41187FB5CF1F|   0|183.17016|\n",
      "|SOBEBDG12A58A76D60|        Kassie Jones|ARI3BMM1187FB4255E|   0|220.78649|\n",
      "|SOILPQQ12AB017E82A|Sohna Nee Sohna Data|AR1ZHYZ1187FB3C717|   0|599.24853|\n",
      "|SOYMRWW12A6D4FAB14|The Moon And I (O...|ARKFYS91187B98E58F|   0| 267.7024|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_table = spark.sql(\"\"\"\n",
    "    select distinct\n",
    "        song_id,\n",
    "        title,\n",
    "        artist_id,\n",
    "        year,\n",
    "        duration\n",
    "    from\n",
    "        song\n",
    "\"\"\")\n",
    "\n",
    "song_table.show(10)\n",
    "\n",
    "try:\n",
    "    song_table.write.partitionBy('year', 'artist_id').parquet(config['LOCAL']['OUTPUT_DATA'] + \"songs.parquet\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Write artist table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+---------------+---------------+----------------+\n",
      "|         artist_id|     artist_name|artist_location|artist_latitude|artist_longitude|\n",
      "+------------------+----------------+---------------+---------------+----------------+\n",
      "|ARPBNLO1187FB3D52F|        Tiny Tim|   New York, NY|       40.71455|       -74.00712|\n",
      "|ARBEBBY1187B9B43DB|       Tom Petty|Gainesville, FL|           null|            null|\n",
      "|AR0IAWL1187B9A96D0|    Danilo Perez|         Panama|         8.4177|       -80.11278|\n",
      "|ARMBR4Y1187B9990EB|    David Martin|California - SF|       37.77916|      -122.42005|\n",
      "|ARD0S291187B9B7BF5|         Rated R|           Ohio|           null|            null|\n",
      "|AR0RCMP1187FB3F427|Billie Jo Spears|   Beaumont, TX|       30.08615|       -94.10158|\n",
      "|ARKRRTF1187B9984DA|Sonora Santanera|               |           null|            null|\n",
      "|ARHHO3O1187B989413|       Bob Azzam|               |           null|            null|\n",
      "|ARJIE2Y1187B994AB7|     Line Renaud|               |           null|            null|\n",
      "|ARGIWFO1187B9B55B7|  Five Bolt Main|               |           null|            null|\n",
      "+------------------+----------------+---------------+---------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artist_table = spark.sql(\"\"\"\n",
    "    select distinct\n",
    "        artist_id,\n",
    "        artist_name,\n",
    "        artist_location,\n",
    "        artist_latitude,\n",
    "        artist_longitude\n",
    "    from\n",
    "        song\n",
    "\"\"\")\n",
    "\n",
    "artist_table.show(10)\n",
    "\n",
    "try:\n",
    "    artist_table.write.parquet(config['LOCAL']['OUTPUT_DATA'] + \"artists.parquet\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Write time table partitioned by year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|         start_time|hour|day|week|month|year|weekday|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|2018-11-15 07:28:47|   7| 15|  46|   11|2018|      3|\n",
      "|2018-11-15 16:49:50|  16| 15|  46|   11|2018|      3|\n",
      "|2018-11-15 17:46:01|  17| 15|  46|   11|2018|      3|\n",
      "|2018-11-21 07:53:00|   7| 21|  47|   11|2018|      2|\n",
      "|2018-11-14 18:35:36|  18| 14|  46|   11|2018|      2|\n",
      "|2018-11-28 11:51:12|  11| 28|  48|   11|2018|      2|\n",
      "|2018-11-28 12:14:27|  12| 28|  48|   11|2018|      2|\n",
      "|2018-11-28 19:00:08|  19| 28|  48|   11|2018|      2|\n",
      "|2018-11-28 19:30:33|  19| 28|  48|   11|2018|      2|\n",
      "|2018-11-05 01:50:53|   1|  5|  45|   11|2018|      0|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table = spark.sql(\"\"\"\n",
    "    select distinct\n",
    "        start_time,\n",
    "        hour(start_time) as hour,\n",
    "        day(start_time) as day,\n",
    "        weekofyear(start_time) as week,\n",
    "        month(start_time) as month,\n",
    "        year(start_time) as year,\n",
    "        weekday(start_time) as weekday\n",
    "    from\n",
    "        log\n",
    "\"\"\")\n",
    "\n",
    "time_table.show(10)\n",
    "\n",
    "try:\n",
    "    time_table.write.partitionBy('year', 'month').parquet(config['LOCAL']['OUTPUT_DATA'] + \"times.parquet\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import shutil\n",
    "#shutil.rmtree(\"spark-warehouse/\")\n",
    "#shutil.rmtree(\"data/extract/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
